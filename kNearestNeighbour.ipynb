{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name : Marcel Zama <br> \n",
    "ID: C00260146 <br> \n",
    "Date: 25/02/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors (kNN) model <br>\n",
    "\n",
    "1. Business Understanding <br>\n",
    "In this program, we are using the standard k-Nearest Neighbors (kNN) algorithm for classification. The standard kNN algorithm is a non-parametric, instance-based learning algorithm that is used for classification and regression tasks. Itâ€™s easy to implement and understand algorithm, but has a major drawback of becoming significantly slows as the size of that data in use grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest Neighbors (kNN) algorithm is suitable for the following scenarios:<br><br>\n",
    "\n",
    "1. Classification and Regression: kNN can be used for both classification and regression tasks. In classification, the algorithm assigns the majority class label among its k-nearest neighbors. In regression, it computes the average (or weighted average) of the target values of its k-nearest neighbors.<br>\n",
    "\n",
    "2. Small to Medium-Sized Datasets: kNN is effective when dealing with small to medium-sized datasets. It does not require training a model beforehand, as it memorizes the entire dataset and uses it during inference. However, this can lead to high memory usage for large datasets.<br>\n",
    "\n",
    "3. Non-Linear Data: kNN can handle non-linear data distributions, making it versatile in various domains. It can capture complex decision boundaries by using appropriate values of k and distance metrics.<br>\n",
    "\n",
    "4. Simple to Implement: kNN is easy to implement and understand. It's a straightforward algorithm that does not require complex assumptions or tuning of hyperparameters.<br>\n",
    "\n",
    "5. Lazy Learning: kNN is an example of lazy learning or instance-based learning. It does not make assumptions about the underlying data distribution and generalizes well to new, unseen data points.<br>\n",
    "\n",
    "6. When Feature Importance is Not Known: kNN does not make any assumptions about the underlying data distribution or feature importance. It treats all features equally, making it suitable when feature importance is not clear or when all features contribute equally to the prediction.<br>\n",
    "\n",
    "7. Anomaly Detection: kNN can be used for anomaly detection tasks, where abnormal data points are identified based on their proximity to neighboring data points. Anomalies are often points that are distant from their k-nearest neighbors.<br>\n",
    "\n",
    "8. Recommendation Systems: In collaborative filtering for recommendation systems, kNN can be used to find similar users or items. For example, in movie recommendations, users who have rated movies similarly in the past may have similar tastes.<br>\n",
    "\n",
    "9. Imbalanced Data: kNN can handle imbalanced datasets where one class is much more prevalent than the others. By adjusting the value of k or using weighted distances, it can give better predictions for minority classes.<br>\n",
    "\n",
    "10. Low Computational Cost During Inference: While kNN can be slow during the training phase (as it memorizes the entire dataset), it is computationally efficient during inference. The prediction time scales linearly with the size of the dataset.<br>\n",
    "\n",
    "11. Human-Level Similarity: kNN can be used in applications where human-level similarity is important, such as image recognition based on visual similarity, recommending similar products based on user preferences, or finding similar documents based on content.<br>\n",
    "\n",
    "In summary, k-Nearest Neighbors (kNN) is a versatile and simple algorithm suitable for small to medium-sized datasets, non-linear data distributions, and scenarios where feature importance is not well-defined. It is effective for classification, regression, anomaly detection, and recommendation systems, among others. However, it may not perform well on high-dimensional data or large datasets due to its computational and memory requirements during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the types of data\n",
    "print(\"Data types:\")\n",
    "print(\"X - Features (first 5 rows):\")\n",
    "print(pd.DataFrame(X, columns=cancer.feature_names).head())\n",
    "print(\"\\ny - Target (first 5 rows):\")\n",
    "print(pd.DataFrame(y, columns=[\"Target\"]).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kNN classifier\n",
    "k = 3  # Number of neighbors\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the kNN model\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MarkDown:\n",
    "In order for the algorithm to work following commands have to be executed.<br> \n",
    "*pip install numpy*<br> \n",
    "*pip install pandas*<br> \n",
    "*pip install scikit-learn*<br> \n",
    "*pip install matplotlib*<br> \n",
    "*pip install seaborn*<br> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
