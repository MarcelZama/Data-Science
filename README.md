# Y4 Portfolio                       Artificial Inteligence 
## Portfolio Content
- ### [About me](https://github.com/MarcelZama/Data-Science/tree/main#about-me-1)
  - ### [Education](https://github.com/MarcelZama/Data-Science/tree/main#education-1)
  - ### [Skills](https://github.com/MarcelZama/Data-Science/tree/main#skills-1)
  - ### [Programming Languages Used](https://github.com/MarcelZama/Data-Science/tree/main#programming-languages-used-1)
  - ### [Projects](https://github.com/MarcelZama/Data-Science/tree/main#projects-1)
- ### [Professional Projects](https://github.com/MarcelZama/Data-Science/tree/main#professional-projects-1)
  - ### [Image Recognition](https://github.com/MarcelZama/Data-Science/tree/main#image-classification)
    - ### [Oportunities](https://github.com/MarcelZama/Data-Science/tree/main#oportunities)
    - ### [Challenges](https://github.com/MarcelZama/Data-Science/tree/main#challenges)
    - ### [Libraries Used](https://github.com/MarcelZama/Data-Science/tree/main#libraries-used)
    - ### [Techniques Used](https://github.com/MarcelZama/Data-Science/tree/main#technologies-used)
    - ### [Tools Used](https://github.com/MarcelZama/Data-Science/tree/main#tools-used)
    - ### [Etihcs](https://github.com/MarcelZama/Data-Science/tree/main#etihcs)
    - ### [Regulations](https://github.com/MarcelZama/Data-Science/tree/main#regulations)
    - ### [GDPR](https://github.com/MarcelZama/Data-Science/tree/main#gdpr)
  - ### [Decision Trees and Random Forests]()
## About me
-  My name is Marcel Zama,
-  I am a Bachelor's degree graduate from South East Technological University Carlow.
-  I am passionate about technology and Nature.

- E-mail: MarcelZama@outlook.com
- [LinkedIn](https://www.linkedin.com/in/marcel-zama-16a174238/)
- [Github](https://github.com/MarcelZama)
## Education
  - Bachelor of Science in Data Science
  - University Name: SETU Carlow
  - Graduation Date: 2024
## Skills
- Ability to Multitask
- Creativity
- Teamwork
- Leadership Skills
- Fast Learner
- Adaptability
## Programming Languages Used
- ### C / C++ / C#
- ### HTML / CSS / JavaScript / PHP 
- ### Java
- ### Python
- ### Assembly
## Projects
Along with my College and Professional career, I worked on many projects on my own time. Projects that I was interested in.
Some of the most interesting projects I have been working on are:

-"Bank Management Software" A project for bank workers that allows them to access and modify data inside the system, and add/delete funds from different accounts.
Also, the workers had an identification number and login and password to access the system, everything was monitored and stored on the database.

-Create a complex data science web app using Python that Collects data automatically from Wikipedia on the world records in sports and creates data tables and graphs.
The project has a very pleasant interface and is easy to use, data is selected by the user and the web app does all the work finding and showing it.

-And many other projects I can not talk about but they were related to Data Science, Game Development, Animation Creation, and Web Development.

More interesting Projects can be found on my Github account.
                                                                              
From my love of technology when I came across AI I was always interested in how they work,
so I did my own research and invested time into creating and explaining how "Image Classification, Natural language processing and Linear Regression" work.

                                                                       
My first project idea was the most popular project, the AI that comes to everyone's mind when they think about Artificial intelligence "Image Classification/Image Recognition".

# Professional Projects
## Image Classification
![Alt Name](https://miro.medium.com/v2/resize:fit:3840/1*oB3S5yHHhvougJkPXuc8og.gif)

Image Classification uses Machine Learning algorithms to detect the presence of objects within a picture and classify the picture.
This is the foundation of Computer Vision and Image recognition, It has led to technological breakthroughs in some of the worldâ€™s most important industries, 
such as the automotive, medical, and manufacturing sectors.
Machines do not analyze a picture as a whole, they analyze the picture by vectors and pixel patterns.

#### //Oportunities//
This project would help me understand the importance and power of Artificial intelligence the way it can be used in day-to-day life and the opportunities it has to offer. 

How does it work?
Images are being scanned by the machine which is tough what is what and then by analyzing a lot of pictures it is training itself, 
the more pictures are scanned by the machine the bigger the chances of the machine to recognize the inside of a picture.
The way in which machines analyze images is very specific. 
The different methods try to mimic the way in which the brain and eyes work in order to suggest an optimal analysis performance.

#### //Challenges//
The hardest part of working on this project was to find the perfect images to feed to the AI, some images might create problems because of their Illumination, 
Background In some cases, View-Point Variation, or Scale Variation factors might confuse the AI.

The Data I decided to use was located on a website specified in image datasets.
https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification

#### *Libraries Used*
*Tensorflow*

TensorFlow is an open-source machine-learning library developed and maintained by Google. It is designed to facilitate the development and deployment of machine learning models,
particularly deep neural networks. TensorFlow is known for its flexibility, scalability, and extensive community support.

*Numpy*

NumPy, short for Numerical Python, is a fundamental open-source library in Python for numerical and mathematical computations. It provides support for working with large,
multi-dimensional arrays and matrices, along with a wide range of mathematical functions to operate on these arrays.
NumPy serves as a foundation for numerous scientific and data analysis libraries in the Python ecosystem.

*Keras*

Keras is like a friendly, easy-to-use tool for building and training computer programs that can learn from data. It's particularly useful for creating things 
like image recognition, language understanding, and other smart features.
#### *Technologies Used*

In this specific project, I used Deep Neural Networks (DNN) and Deep learning:
Deep neural networks are a subset of machine learning algorithms that have been around since the 1950s. DNNs can perform tasks like image recognition, speech recognition, and natural language processing. They consist of multiple hidden layers of neurons where each layer learns a representation of its input data. These representations are then used to make predictions about the output data.

Deep learning is a subset of machine learning that uses multiple processing layers (usually hundreds) to learn data representations. This allows computers to perform tasks that are difficult for humans. Deep learning has been used in many fields, including computer vision, speech recognition, natural language processing, robotics, and reinforcement learning.

#### *Techniques Used*

Decision-making in machine learning algorithms involves the process by which models make predictions or classifications based on input data. Different algorithms use various mechanisms for decision-making, and the choice of algorithm often depends on the nature of the problem and the characteristics of the data. The key aspects of decision-making in machine learning are:
1. Training Phase:
  During the training phase, a machine learning model learns patterns and relationships in the input data. The model is exposed to labeled examples (input-output pairs), and it adjusts its internal parameters to minimize the difference between predicted outputs and actual outputs.
2. Neural Networks:
  Neural networks consist of layers of interconnected nodes (neurons). Each connection has a weight, and the network makes decisions through a series of weighted computations and activations. The final layer's output represents the decision.
3. Probabilistic Models:
  Some models, like logistic regression and Naive Bayes, provide probabilistic outputs. Decision-making involves thresholding these probabilities to make a binary decision or selecting the class with the highest probability.
4. Reinforcement Learning
  In reinforcement learning, an agent learns to make decisions by interacting with an environment. The agent receives feedback (rewards or penalties) based on its decisions, and over time, it learns to make decisions that maximize cumulative rewards.
5. Explainability:
  Interpretable models, such as decision trees, provide explicit rules for decision-making. For complex models like neural networks, interpretability tools may be used to understand how decisions are influenced by input features.
6. Monitoring and Evaluation:
   Continuous monitoring and evaluation of model performance are essential for decision-making in real-world applications. If a model's performance degrades or biases are detected, adjustments may be needed.  
Linear Regression
The term regression is used when you try to find the relationship between variables.

In Machine Learning and in statistical modeling, this relationship is used to predict the outcome of events.

#### *Tools Used*
*Python*

Python is a high-level, versatile, and widely used programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991.
Python has gained popularity across a wide range of domains, including web development, data science, machine learning, scientific computing, automation, and more. 
https://www.python.org/

*Visual Studio Code*

Visual Studio Code (VS Code) is a free, open-source, and highly popular source code editor developed by Microsoft.
It is widely used by developers for a variety of programming and development tasks across different platforms. 
https://code.visualstudio.com/

#### *Etihcs*

Ethics in data science and machine learning are critical aspects that involve responsible and fair practices in the collection, analysis, and application of data. Here are key considerations related to ethics in these fields:
1. Privacy and Data Protection:
  1.1. Concern: Unauthorized access, use, or sharing of personal or sensitive information.
  1.2. Practice: Implement strong data protection measures, anonymize or pseudonymize data when possible, and ensure compliance with relevant data protection laws (e.g., GDPR).
2. Transparency and Explainability:
  2.1. Concern: Lack of transparency in how models make decisions.
  2.2. Practice: Strive for transparent models, provide explanations for model predictions, and enable users to understand the factors influencing the outcomes.
3. Informed Consent:
  3.1. Concern: Lack of informed consent from individuals whose data is used.
  3.2. Practice: Obtain clear and informed consent when collecting and using data, communicate the purpose of data usage, and allow individuals to opt-out if possible.
4. Accountability and Responsibility:
  4.1. Concern: Lack of accountability for the consequences of model decisions.
  4.2. Practice: Clearly define responsibilities, establish accountability for model performance, and be transparent about the limitations and potential risks associated with the models.
5. Security:
  5.1. Concern: Security vulnerabilities that may lead to data breaches or misuse.
  5.2 Practice: Implement robust security measures to protect data, regularly update software and systems, and conduct security audits to identify and address potential vulnerabilities. 
Ethical considerations in data science and machine learning are dynamic and evolving, requiring ongoing efforts to adapt to new challenges and technologies. Practitioners, researchers, and organizations should prioritize ethical considerations to ensure the responsible and positive impact of their work on individuals and society.

#### *Regulations*
Regulations related to data science and machine learning vary by country and region.
Here are some general considerations and examples of regulations that were relevant to the subject:

1. General Data Protection Regulation (GDPR):

GDPR is a comprehensive data protection regulation in the European Union (EU). It regulates the processing of personal data and imposes strict requirements on organizations that collect, process or store personal data.

2. Sector-Specific Regulations:

Depending on the industry, there may be sector-specific regulations governing the use of data and machine learning. For example, financial institutions may be subject to regulations like Basel III or MiFID II.

3. Ethical Guidelines and Standards:

While not legally binding, ethical guidelines and standards, such as those provided by organizations like the Institute of Electrical and Electronics Engineers (IEEE) and the Association for Computing Machinery (ACM), offer principles for responsible and ethical AI and machine learning development.

#### *GDPR*

The General Data Protection Regulation (GDPR) is a comprehensive data protection law in the European Union (EU) that aims to give individuals more control over their personal data. When it comes to machine learning (ML) and data science activities, GDPR imposes certain obligations and considerations. Here are key points related to GDPR compliance in the context of machine learning and data science:

1. Lawful Basis for Processing:

Data processing, including that involved in machine learning models, must have a lawful basis. Consent, contractual necessity, legal obligations, vital interests, public tasks, and legitimate interests are common lawful bases.

2. Transparency and Fair Processing:

Inform individuals about how their data will be used in a transparent manner. This includes explaining the purpose of data processing, the types of data collected, and any automated decision-making, such as that involved in machine learning models.

3. Data Minimization:

Collect and process only the data that is necessary for the intended purpose. Avoid collecting excessive or irrelevant data.

4. Purpose Limitation:

Process data only for the specific purposes for which it was collected. If new purposes arise, ensure that they are compatible with the original purposes.

5. Data Accuracy:

Ensure that the data used in machine learning models is accurate and up-to-date. Take steps to correct or erase inaccurate data.

6. Storage Limitation:

Do not retain personal data for longer than necessary. Define and adhere to specific data retention periods.

7. Individual Rights:

Respect individuals' rights under GDPR, including the right to access, rectification, erasure (right to be forgotten), and data portability. Provide mechanisms for individuals to exercise these rights.

8. Automated Decision-Making:

If machine learning models result in automated decision-making, including profiling, individuals have the right not to be subject to decisions based solely on automated processing. Provide explanations and allow for human intervention.

9. Data Security:

Implement appropriate security measures to protect personal data from unauthorized access, disclosure, alteration, and destruction. This is crucial, especially when dealing with sensitive data in machine learning.

10. Data Protection Impact Assessments (DPIAs):

Conduct DPIAs for high-risk processing activities, such as the use of new technologies or large-scale profiling, to assess and mitigate risks to data subjects.

11. Data Processing Records:

Maintain records of data processing activities, including those related to machine learning, to demonstrate compliance with GDPR requirements.

12. Data Protection Officer (DPO):

Appoint a Data Protection Officer if your organization's core activities involve large-scale processing of sensitive data.

It's essential for organizations involved in machine learning and data science to integrate GDPR compliance into their processes and practices. This involves collaboration between data scientists, legal teams, and privacy professionals to ensure that both the technical and legal aspects are addressed appropriately.                                                                    
                                                                       *Natural language processing*
                                                                      
                                                                       *Linear Regression*
